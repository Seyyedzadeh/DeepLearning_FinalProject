{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "from colorama import Fore\n",
    "import locale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'English_United States.1252'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Common Definitions\n",
    "locale.setlocale(\n",
    "    category=locale.LC_ALL,\n",
    "    locale = '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37mDataSet 'mevlana' has contains \u001b[32m339,019\u001b[37m characters.\n"
     ]
    }
   ],
   "source": [
    "# بارگیری دیتاست\n",
    "DataSet = (\"mevlana\", \"molana_shams.txt\")\n",
    "with open(\n",
    "        DataSet[1],\n",
    "        mode = \"r\",\n",
    "        encoding = \"utf-8\"\n",
    "    ) as f:\n",
    "    CleanText = f.read()\n",
    "\n",
    "print(f\"{Fore.WHITE}DataSet '{DataSet[0]}' has contains {Fore.GREEN}{len(Text):n}{Fore.WHITE} characters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37mDataSet 'mevlana' has contains \u001b[32m35\u001b[37m unique characters:\n",
      "{'\\n': 0, ' ': 1, 'ا': 2, 'ب': 3, 'ت': 4, 'ث': 5, 'ج': 6, 'ح': 7, 'خ': 8, 'د': 9, 'ذ': 10, 'ر': 11, 'ز': 12, 'س': 13, 'ش': 14, 'ص': 15, 'ض': 16, 'ط': 17, 'ظ': 18, 'ع': 19, 'غ': 20, 'ف': 21, 'ق': 22, 'ل': 23, 'م': 24, 'ن': 25, 'ه': 26, 'و': 27, 'ٖ': 28, 'پ': 29, 'چ': 30, 'ژ': 31, 'ک': 32, 'گ': 33, 'ی': 34}\n"
     ]
    }
   ],
   "source": [
    "UniqueChars = sorted(set(CleanText))\n",
    "# UniqueChars = \" ءأئؤابپتثجچحخدذرزژسشصضطظعغفقکگلمنوهی\"\n",
    "Vocabulary = {char : index for index, char in enumerate(UniqueChars)}\n",
    "CharacterMap = np.array(UniqueChars)\n",
    "VocabularySize = len(Vocabulary)\n",
    "print(f\"{Fore.WHITE}DataSet '{DataSet[0]}' has contains {Fore.GREEN}{VocabularySize:n}{Fore.WHITE} unique characters:\")\n",
    "print(Vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ای رستخیز ناگهان وی رحمت بیمنتها\n",
      "ای اتشی افروخته در بیشه اندیشهها\n",
      "\n",
      "امروز خ\n",
      "=>\n",
      "[ 2 34  1 11 13  4  8 34 12  1 25  2 33 26  2 25  1 27 34  1 11  7 24  4\n",
      "  1  3 34 24 25  4 26  2  0  2 34  1  2  4 14 34  1  2 21 11 27  8  4 26\n",
      "  1  9 11  1  3 34 14 26  1  2 25  9 34 14 26 26  2  0  0  2 24 11 27 12\n",
      "  1  8]\n"
     ]
    }
   ],
   "source": [
    "#Convert Text => Integer codded vector\n",
    "CoddedText = np.array([Vocabulary[c] for c in CleanText if c in Vocabulary])\n",
    "print(f\"{CleanText[:74]}\\n=>\\n{CoddedText[:74]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# تعریف داده‌های آموزشی و تست\n",
    "SequenceLen = 100\n",
    "ExamplesPerEpoch = len(CleanText) // SequenceLen\n",
    "\n",
    "tsDataSet = tf.data.Dataset.from_tensor_slices(CoddedText)\n",
    "Sequences = tsDataSet.batch(SequenceLen + 1, drop_remainder = True)\n",
    "\n",
    "# تعریف تابع برای تبدیل داده به بردارهای آماده برای ورود به مدل\n",
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "tsDataSet = Sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# تعریف مدل\n",
    "EmbeddingDimension = 256\n",
    "RnningUnits = 1024\n",
    "BatchSize = 64\n",
    "\n",
    "def build_model(embedding_dim, rnn_units, batch_size):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(VocabularySize, embedding_dim, batch_input_shape=[batch_size, None]),\n",
    "        tf.keras.layers.LSTM(\n",
    "            rnn_units,\n",
    "            return_sequences = True,\n",
    "            stateful = True,\n",
    "            recurrent_initializer = 'glorot_uniform'),\n",
    "        tf.keras.layers.Dense(VocabularySize)\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "model = build_model(EmbeddingDimension, RnningUnits, BatchSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# تعریف تابعی برای تولید شعر با استفاده از مدل\n",
    "def generate_text(model, seed):\n",
    "    # تعریف تعداد کاراکترهای تولیدی\n",
    "    num_generate = 300\n",
    "\n",
    "    # تبدیل کاراکترهای ورودی به بردار اعداد\n",
    "    input_eval = [Vocabulary[s] for s in seed]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "    # تعریف شعر تولیدی\n",
    "    GeneratedText = []\n",
    "\n",
    "    # دما برای تعیین تنوع در خروجی\n",
    "    temperature = 1.0\n",
    "\n",
    "    # تولید کاراکترهای جدید با استفاده از مدل LSTM\n",
    "    model.reset_states()\n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "        # تعیین احتمالات کاراکترهای بعدی با در نظر گرفتن دما\n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples = 1)[-1,0].numpy()\n",
    "\n",
    "        # ادام نتیجه شده را به شکل کاراکتر تبدیل کرده و به شعر تولیدی اضافه می‌کنیم\n",
    "        GeneratedText.append(CharacterMap[predicted_id])\n",
    "\n",
    "        # استفاده از کاراکتر تولید شده بعنوان ورودی برای کاراکتر بعدی\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return (seed + ''.join(GeneratedText))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      "52/52 [==============================] - 63s 1s/step - loss: 2.9648\n",
      "Epoch 2/250\n",
      "52/52 [==============================] - 63s 1s/step - loss: 2.5896\n",
      "Epoch 3/250\n",
      "52/52 [==============================] - 67s 1s/step - loss: 2.4675\n",
      "Epoch 4/250\n",
      "52/52 [==============================] - 68s 1s/step - loss: 2.3804\n",
      "Epoch 5/250\n",
      "52/52 [==============================] - 68s 1s/step - loss: 2.3083\n",
      "Epoch 6/250\n",
      "52/52 [==============================] - 69s 1s/step - loss: 2.2522\n",
      "Epoch 7/250\n",
      "52/52 [==============================] - 70s 1s/step - loss: 2.2040\n",
      "Epoch 8/250\n",
      "52/52 [==============================] - 68s 1s/step - loss: 2.1585\n",
      "Epoch 9/250\n",
      "52/52 [==============================] - 69s 1s/step - loss: 2.1159\n",
      "Epoch 10/250\n",
      "52/52 [==============================] - 68s 1s/step - loss: 2.0739\n",
      "Epoch 11/250\n",
      "52/52 [==============================] - 68s 1s/step - loss: 2.0331\n",
      "Epoch 12/250\n",
      "52/52 [==============================] - 68s 1s/step - loss: 1.9929\n",
      "Epoch 13/250\n",
      "52/52 [==============================] - 68s 1s/step - loss: 1.9529\n",
      "Epoch 14/250\n",
      "52/52 [==============================] - 68s 1s/step - loss: 1.9126\n",
      "Epoch 15/250\n",
      "52/52 [==============================] - 68s 1s/step - loss: 1.8697\n",
      "Epoch 16/250\n",
      "52/52 [==============================] - 68s 1s/step - loss: 1.8237\n",
      "Epoch 17/250\n",
      "52/52 [==============================] - 68s 1s/step - loss: 1.7736\n",
      "Epoch 18/250\n",
      "52/52 [==============================] - 68s 1s/step - loss: 1.7168\n",
      "Epoch 19/250\n",
      "52/52 [==============================] - 68s 1s/step - loss: 1.6507\n",
      "Epoch 20/250\n",
      "52/52 [==============================] - 67s 1s/step - loss: 1.5753\n",
      "Epoch 21/250\n",
      "52/52 [==============================] - 68s 1s/step - loss: 1.4940\n",
      "Epoch 22/250\n",
      "52/52 [==============================] - 68s 1s/step - loss: 1.4099\n",
      "Epoch 23/250\n",
      "52/52 [==============================] - 68s 1s/step - loss: 1.3261\n",
      "Epoch 24/250\n",
      "52/52 [==============================] - 68s 1s/step - loss: 1.2393\n",
      "Epoch 25/250\n",
      "52/52 [==============================] - 68s 1s/step - loss: 1.1481\n",
      "Epoch 26/250\n",
      "52/52 [==============================] - 110s 2s/step - loss: 1.0623\n",
      "Epoch 27/250\n",
      "52/52 [==============================] - 193s 4s/step - loss: 0.9821\n",
      "Epoch 28/250\n",
      "52/52 [==============================] - 193s 4s/step - loss: 0.9015\n",
      "Epoch 29/250\n",
      "52/52 [==============================] - 194s 4s/step - loss: 0.8286\n",
      "Epoch 30/250\n",
      "52/52 [==============================] - 193s 4s/step - loss: 0.7572\n",
      "Epoch 31/250\n",
      "52/52 [==============================] - 190s 4s/step - loss: 0.6941\n",
      "Epoch 32/250\n",
      "52/52 [==============================] - 194s 4s/step - loss: 0.6345\n",
      "Epoch 33/250\n",
      "52/52 [==============================] - 191s 4s/step - loss: 0.5781\n",
      "Epoch 34/250\n",
      "52/52 [==============================] - 194s 4s/step - loss: 0.5240\n",
      "Epoch 35/250\n",
      "52/52 [==============================] - 196s 4s/step - loss: 0.4767\n",
      "Epoch 36/250\n",
      "52/52 [==============================] - 194s 4s/step - loss: 0.4273\n",
      "Epoch 37/250\n",
      "52/52 [==============================] - 195s 4s/step - loss: 0.3791\n",
      "Epoch 38/250\n",
      "52/52 [==============================] - 171s 3s/step - loss: 0.3368\n",
      "Epoch 39/250\n",
      "52/52 [==============================] - 64s 1s/step - loss: 0.3025\n",
      "Epoch 40/250\n",
      "52/52 [==============================] - 62s 1s/step - loss: 0.2701\n",
      "Epoch 41/250\n",
      "52/52 [==============================] - 66s 1s/step - loss: 0.2380\n",
      "Epoch 42/250\n",
      "52/52 [==============================] - 68s 1s/step - loss: 0.2114\n",
      "Epoch 43/250\n",
      "52/52 [==============================] - 67s 1s/step - loss: 0.1867\n",
      "Epoch 44/250\n",
      "52/52 [==============================] - 68s 1s/step - loss: 0.1654\n",
      "Epoch 45/250\n",
      "52/52 [==============================] - 66s 1s/step - loss: 0.1469\n",
      "Epoch 46/250\n",
      "52/52 [==============================] - 67s 1s/step - loss: 0.1301\n",
      "Epoch 47/250\n",
      "52/52 [==============================] - 67s 1s/step - loss: 0.1141\n",
      "Epoch 48/250\n",
      "52/52 [==============================] - 118s 2s/step - loss: 0.1000\n",
      "Epoch 49/250\n",
      "52/52 [==============================] - 139s 3s/step - loss: 0.0867\n",
      "Epoch 50/250\n",
      "52/52 [==============================] - 172s 3s/step - loss: 0.0751\n",
      "Epoch 51/250\n",
      "52/52 [==============================] - 173s 3s/step - loss: 0.0650\n",
      "Epoch 52/250\n",
      "52/52 [==============================] - 172s 3s/step - loss: 0.0562\n",
      "Epoch 53/250\n",
      "52/52 [==============================] - 175s 3s/step - loss: 0.0483\n",
      "Epoch 54/250\n",
      "52/52 [==============================] - 175s 3s/step - loss: 0.0411\n",
      "Epoch 55/250\n",
      "52/52 [==============================] - 174s 3s/step - loss: 0.0355\n",
      "Epoch 56/250\n",
      "52/52 [==============================] - 175s 3s/step - loss: 0.0307\n",
      "Epoch 57/250\n",
      "52/52 [==============================] - 83s 2s/step - loss: 0.0266\n",
      "Epoch 58/250\n",
      "52/52 [==============================] - 93s 2s/step - loss: 0.0232\n",
      "Epoch 59/250\n",
      "52/52 [==============================] - 64s 1s/step - loss: 0.0203\n",
      "Epoch 60/250\n",
      "52/52 [==============================] - 67s 1s/step - loss: 0.0178\n",
      "Epoch 61/250\n",
      "52/52 [==============================] - 67s 1s/step - loss: 0.0155\n",
      "Epoch 62/250\n",
      "52/52 [==============================] - 66s 1s/step - loss: 0.0135\n",
      "Epoch 63/250\n",
      "52/52 [==============================] - 66s 1s/step - loss: 0.0118\n",
      "Epoch 64/250\n",
      "52/52 [==============================] - 67s 1s/step - loss: 0.0103\n",
      "Epoch 65/250\n",
      "52/52 [==============================] - 67s 1s/step - loss: 0.0091\n",
      "Epoch 66/250\n",
      "52/52 [==============================] - 67s 1s/step - loss: 0.0081\n",
      "Epoch 67/250\n",
      "52/52 [==============================] - 67s 1s/step - loss: 0.0073\n",
      "Epoch 68/250\n",
      "52/52 [==============================] - 67s 1s/step - loss: 0.0067\n",
      "Epoch 69/250\n",
      "52/52 [==============================] - 67s 1s/step - loss: 0.0061\n",
      "Epoch 70/250\n",
      "52/52 [==============================] - 67s 1s/step - loss: 0.0056\n",
      "Epoch 71/250\n",
      "52/52 [==============================] - 67s 1s/step - loss: 0.0052\n",
      "Epoch 72/250\n",
      "52/52 [==============================] - 67s 1s/step - loss: 0.0048\n",
      "Epoch 73/250\n",
      "52/52 [==============================] - 67s 1s/step - loss: 0.0045\n",
      "Epoch 74/250\n",
      "52/52 [==============================] - 67s 1s/step - loss: 0.0042\n",
      "Epoch 75/250\n",
      "52/52 [==============================] - 67s 1s/step - loss: 0.0040\n",
      "Epoch 76/250\n",
      "52/52 [==============================] - 67s 1s/step - loss: 0.0038\n",
      "Epoch 77/250\n",
      "52/52 [==============================] - 67s 1s/step - loss: 0.0037\n",
      "Epoch 78/250\n",
      "52/52 [==============================] - 67s 1s/step - loss: 0.0036\n",
      "Epoch 79/250\n",
      "52/52 [==============================] - 68s 1s/step - loss: 0.0038\n",
      "Epoch 80/250\n",
      "52/52 [==============================] - 67s 1s/step - loss: 0.0037\n",
      "Epoch 81/250\n",
      "52/52 [==============================] - 67s 1s/step - loss: 0.0036\n",
      "Epoch 82/250\n",
      "52/52 [==============================] - 67s 1s/step - loss: 0.0033\n",
      "Epoch 83/250\n",
      "52/52 [==============================] - 67s 1s/step - loss: 0.0031\n",
      "Epoch 84/250\n",
      "52/52 [==============================] - 67s 1s/step - loss: 0.0032\n",
      "Epoch 85/250\n",
      "52/52 [==============================] - 67s 1s/step - loss: 0.0031\n",
      "Epoch 86/250\n",
      "52/52 [==============================] - 67s 1s/step - loss: 0.0031\n",
      "Epoch 87/250\n",
      "52/52 [==============================] - 67s 1s/step - loss: 0.0030\n",
      "Epoch 88/250\n",
      "52/52 [==============================] - 67s 1s/step - loss: 0.0036\n",
      "Epoch 89/250\n",
      "52/52 [==============================] - 67s 1s/step - loss: 0.0043\n",
      "Epoch 90/250\n",
      "52/52 [==============================] - 67s 1s/step - loss: 0.0054\n",
      "Epoch 91/250\n",
      "52/52 [==============================] - 67s 1s/step - loss: 0.0150\n",
      "Epoch 92/250\n",
      "52/52 [==============================] - 67s 1s/step - loss: 0.3004\n",
      "Epoch 93/250\n",
      "52/52 [==============================] - 67s 1s/step - loss: 0.7427\n",
      "Epoch 94/250\n",
      "52/52 [==============================] - 67s 1s/step - loss: 0.4420\n",
      "Epoch 95/250\n",
      "52/52 [==============================] - 67s 1s/step - loss: 0.2428\n",
      "Epoch 96/250\n",
      "52/52 [==============================] - 67s 1s/step - loss: 0.1443\n",
      "Epoch 97/250\n",
      "52/52 [==============================] - 67s 1s/step - loss: 0.0923\n",
      "Epoch 98/250\n",
      "52/52 [==============================] - 67s 1s/step - loss: 0.0624\n",
      "Epoch 99/250\n",
      "52/52 [==============================] - 67s 1s/step - loss: 0.0450\n",
      "Epoch 100/250\n",
      "52/52 [==============================] - 67s 1s/step - loss: 0.0334\n",
      "Epoch 101/250\n",
      "52/52 [==============================] - 67s 1s/step - loss: 0.0245\n",
      "Epoch 102/250\n",
      "52/52 [==============================] - 144s 3s/step - loss: 0.0185\n",
      "Epoch 103/250\n",
      "52/52 [==============================] - 162s 3s/step - loss: 0.0140\n",
      "Epoch 104/250\n",
      "52/52 [==============================] - 169s 3s/step - loss: 0.0107\n",
      "Epoch 105/250\n",
      "52/52 [==============================] - 173s 3s/step - loss: 0.0082\n",
      "Epoch 106/250\n",
      "52/52 [==============================] - 164s 3s/step - loss: 0.0065\n",
      "Epoch 107/250\n",
      "52/52 [==============================] - 101s 2s/step - loss: 0.0054\n",
      "Epoch 108/250\n",
      "52/52 [==============================] - 130s 3s/step - loss: 0.0047\n",
      "Epoch 109/250\n",
      "52/52 [==============================] - 160s 3s/step - loss: 0.0043\n",
      "Epoch 110/250\n",
      "52/52 [==============================] - 141s 3s/step - loss: 0.0039\n",
      "Epoch 111/250\n",
      "52/52 [==============================] - 151s 3s/step - loss: 0.0036\n",
      "Epoch 112/250\n",
      "52/52 [==============================] - 165s 3s/step - loss: 0.0034\n",
      "Epoch 113/250\n",
      "52/52 [==============================] - 136s 3s/step - loss: 0.0032\n",
      "Epoch 114/250\n",
      "52/52 [==============================] - 62s 1s/step - loss: 0.0030\n",
      "Epoch 115/250\n",
      "52/52 [==============================] - 67s 1s/step - loss: 0.0028\n",
      "Epoch 116/250\n",
      "52/52 [==============================] - 67s 1s/step - loss: 0.0027\n",
      "Epoch 117/250\n",
      "52/52 [==============================] - 67s 1s/step - loss: 0.0025\n",
      "Epoch 118/250\n",
      "52/52 [==============================] - 69s 1s/step - loss: 0.0024\n",
      "Epoch 119/250\n",
      "52/52 [==============================] - 134s 3s/step - loss: 0.0023\n",
      "Epoch 120/250\n",
      "52/52 [==============================] - 163s 3s/step - loss: 0.0022\n",
      "Epoch 121/250\n",
      "52/52 [==============================] - 172s 3s/step - loss: 0.0021\n",
      "Epoch 122/250\n",
      "52/52 [==============================] - 174s 3s/step - loss: 0.0020\n",
      "Epoch 123/250\n",
      "52/52 [==============================] - 178s 3s/step - loss: 0.0019\n",
      "Epoch 124/250\n",
      "52/52 [==============================] - 176s 3s/step - loss: 0.0018\n",
      "Epoch 125/250\n",
      "52/52 [==============================] - 176s 3s/step - loss: 0.0018\n",
      "Epoch 126/250\n",
      "52/52 [==============================] - 132s 3s/step - loss: 0.0017\n",
      "Epoch 127/250\n",
      "52/52 [==============================] - 61s 1s/step - loss: 0.0016\n",
      "Epoch 128/250\n",
      "52/52 [==============================] - 63s 1s/step - loss: 0.0015\n",
      "Epoch 129/250\n",
      "52/52 [==============================] - 66s 1s/step - loss: 0.0015\n",
      "Epoch 130/250\n",
      "52/52 [==============================] - 66s 1s/step - loss: 0.0014\n",
      "Epoch 131/250\n",
      "52/52 [==============================] - 66s 1s/step - loss: 0.0014\n",
      "Epoch 132/250\n",
      "52/52 [==============================] - 66s 1s/step - loss: 0.0014\n",
      "Epoch 133/250\n",
      "52/52 [==============================] - 66s 1s/step - loss: 0.0014\n",
      "Epoch 134/250\n",
      "52/52 [==============================] - 66s 1s/step - loss: 0.0013\n",
      "Epoch 135/250\n",
      "52/52 [==============================] - 66s 1s/step - loss: 0.0012\n",
      "Epoch 136/250\n",
      "52/52 [==============================] - 67s 1s/step - loss: 0.0012\n",
      "Epoch 137/250\n",
      "52/52 [==============================] - 66s 1s/step - loss: 0.0011\n",
      "Epoch 138/250\n",
      "52/52 [==============================] - 103s 2s/step - loss: 0.0011\n",
      "Epoch 139/250\n",
      "52/52 [==============================] - 81s 2s/step - loss: 0.0011\n",
      "Epoch 140/250\n",
      "52/52 [==============================] - 71s 1s/step - loss: 0.0010\n",
      "Epoch 141/250\n",
      "52/52 [==============================] - 69s 1s/step - loss: 9.6754e-04\n",
      "Epoch 142/250\n",
      "52/52 [==============================] - 68s 1s/step - loss: 9.2534e-04\n",
      "Epoch 143/250\n",
      "52/52 [==============================] - 101s 2s/step - loss: 9.7772e-04\n",
      "Epoch 144/250\n",
      "52/52 [==============================] - 103s 2s/step - loss: 0.0010\n",
      "Epoch 145/250\n",
      "52/52 [==============================] - 103s 2s/step - loss: 0.0011\n",
      "Epoch 146/250\n",
      "52/52 [==============================] - 103s 2s/step - loss: 0.0010\n",
      "Epoch 147/250\n",
      "52/52 [==============================] - 104s 2s/step - loss: 0.0010\n",
      "Epoch 148/250\n",
      "52/52 [==============================] - 281s 5s/step - loss: 0.0012\n",
      "Epoch 149/250\n",
      "52/52 [==============================] - 356s 7s/step - loss: 0.0014\n",
      "Epoch 150/250\n",
      "52/52 [==============================] - 358s 7s/step - loss: 0.0017\n",
      "Epoch 151/250\n",
      "52/52 [==============================] - 354s 7s/step - loss: 0.0025\n",
      "Epoch 152/250\n",
      "52/52 [==============================] - 353s 7s/step - loss: 0.0059\n",
      "Epoch 153/250\n",
      "52/52 [==============================] - 315s 6s/step - loss: 0.1202\n",
      "Epoch 154/250\n",
      "52/52 [==============================] - 90s 2s/step - loss: 0.9508\n",
      "Epoch 155/250\n",
      "52/52 [==============================] - 104s 2s/step - loss: 0.7028\n",
      "Epoch 156/250\n",
      "52/52 [==============================] - 104s 2s/step - loss: 0.3785\n",
      "Epoch 157/250\n",
      "52/52 [==============================] - 104s 2s/step - loss: 0.2114\n",
      "Epoch 158/250\n",
      "52/52 [==============================] - 105s 2s/step - loss: 0.1282\n",
      "Epoch 159/250\n",
      "52/52 [==============================] - 104s 2s/step - loss: 0.0828\n",
      "Epoch 160/250\n",
      "52/52 [==============================] - 105s 2s/step - loss: 0.0562\n",
      "Epoch 161/250\n",
      "52/52 [==============================] - 105s 2s/step - loss: 0.0390\n",
      "Epoch 162/250\n",
      "52/52 [==============================] - 104s 2s/step - loss: 0.0276\n",
      "Epoch 163/250\n",
      "52/52 [==============================] - 104s 2s/step - loss: 0.0195\n",
      "Epoch 164/250\n",
      "52/52 [==============================] - 104s 2s/step - loss: 0.0142\n",
      "Epoch 165/250\n",
      "52/52 [==============================] - 104s 2s/step - loss: 0.0102\n",
      "Epoch 166/250\n",
      "52/52 [==============================] - 104s 2s/step - loss: 0.0074\n",
      "Epoch 167/250\n",
      "52/52 [==============================] - 104s 2s/step - loss: 0.0059\n",
      "Epoch 168/250\n",
      "52/52 [==============================] - 110s 2s/step - loss: 0.0052\n",
      "Epoch 169/250\n",
      "52/52 [==============================] - 112s 2s/step - loss: 0.0046\n",
      "Epoch 170/250\n",
      "52/52 [==============================] - 101s 2s/step - loss: 0.0042\n",
      "Epoch 171/250\n",
      "52/52 [==============================] - 102s 2s/step - loss: 0.0039\n",
      "Epoch 172/250\n",
      "52/52 [==============================] - 102s 2s/step - loss: 0.0036\n",
      "Epoch 173/250\n",
      "52/52 [==============================] - 102s 2s/step - loss: 0.0034\n",
      "Epoch 174/250\n",
      "52/52 [==============================] - 102s 2s/step - loss: 0.0032\n",
      "Epoch 175/250\n",
      "52/52 [==============================] - 102s 2s/step - loss: 0.0030\n",
      "Epoch 176/250\n",
      "52/52 [==============================] - 103s 2s/step - loss: 0.0028\n",
      "Epoch 177/250\n",
      "52/52 [==============================] - 102s 2s/step - loss: 0.0026\n",
      "Epoch 178/250\n",
      "52/52 [==============================] - 103s 2s/step - loss: 0.0025\n",
      "Epoch 179/250\n",
      "52/52 [==============================] - 101s 2s/step - loss: 0.0024\n",
      "Epoch 180/250\n",
      "52/52 [==============================] - 102s 2s/step - loss: 0.0023\n",
      "Epoch 181/250\n",
      "52/52 [==============================] - 102s 2s/step - loss: 0.0022\n",
      "Epoch 182/250\n",
      "52/52 [==============================] - 102s 2s/step - loss: 0.0021\n",
      "Epoch 183/250\n",
      "52/52 [==============================] - 102s 2s/step - loss: 0.0020\n",
      "Epoch 184/250\n",
      "52/52 [==============================] - 103s 2s/step - loss: 0.0019\n",
      "Epoch 185/250\n",
      "52/52 [==============================] - 102s 2s/step - loss: 0.0018\n",
      "Epoch 186/250\n",
      "52/52 [==============================] - 103s 2s/step - loss: 0.0017\n",
      "Epoch 187/250\n",
      "52/52 [==============================] - 102s 2s/step - loss: 0.0017\n",
      "Epoch 188/250\n",
      "52/52 [==============================] - 102s 2s/step - loss: 0.0016\n",
      "Epoch 189/250\n",
      "52/52 [==============================] - 104s 2s/step - loss: 0.0015\n",
      "Epoch 190/250\n",
      "52/52 [==============================] - 103s 2s/step - loss: 0.0015\n",
      "Epoch 191/250\n",
      "52/52 [==============================] - 102s 2s/step - loss: 0.0014\n",
      "Epoch 192/250\n",
      "52/52 [==============================] - 102s 2s/step - loss: 0.0014\n",
      "Epoch 193/250\n",
      "52/52 [==============================] - 104s 2s/step - loss: 0.0013\n",
      "Epoch 194/250\n",
      "52/52 [==============================] - 103s 2s/step - loss: 0.0013\n",
      "Epoch 195/250\n",
      "52/52 [==============================] - 106s 2s/step - loss: 0.0012\n",
      "Epoch 196/250\n",
      "52/52 [==============================] - 106s 2s/step - loss: 0.0012\n",
      "Epoch 197/250\n",
      "52/52 [==============================] - 2099s 41s/step - loss: 0.0011\n",
      "Epoch 198/250\n",
      "52/52 [==============================] - 91s 2s/step - loss: 0.0011\n",
      "Epoch 199/250\n",
      "52/52 [==============================] - 65s 1s/step - loss: 0.0011\n",
      "Epoch 200/250\n",
      "52/52 [==============================] - 68s 1s/step - loss: 0.0010\n",
      "Epoch 201/250\n",
      "52/52 [==============================] - 124s 2s/step - loss: 9.8801e-04\n",
      "Epoch 202/250\n",
      "52/52 [==============================] - 155s 3s/step - loss: 9.5504e-04\n",
      "Epoch 203/250\n",
      "52/52 [==============================] - 150s 3s/step - loss: 9.2282e-04\n",
      "Epoch 204/250\n",
      "52/52 [==============================] - 168s 3s/step - loss: 8.9255e-04\n",
      "Epoch 205/250\n",
      "52/52 [==============================] - 169s 3s/step - loss: 8.6290e-04\n",
      "Epoch 206/250\n",
      "52/52 [==============================] - 166s 3s/step - loss: 8.3501e-04\n",
      "Epoch 207/250\n",
      "52/52 [==============================] - 166s 3s/step - loss: 8.0765e-04\n",
      "Epoch 208/250\n",
      "52/52 [==============================] - 167s 3s/step - loss: 7.8192e-04\n",
      "Epoch 209/250\n",
      "52/52 [==============================] - 155s 3s/step - loss: 7.5662e-04\n",
      "Epoch 210/250\n",
      "52/52 [==============================] - 95s 2s/step - loss: 7.3283e-04\n",
      "Epoch 211/250\n",
      "52/52 [==============================] - 165s 3s/step - loss: 7.0941e-04\n",
      "Epoch 212/250\n",
      "52/52 [==============================] - 164s 3s/step - loss: 6.8739e-04\n",
      "Epoch 213/250\n",
      "52/52 [==============================] - 147s 3s/step - loss: 6.6568e-04\n",
      "Epoch 214/250\n",
      "52/52 [==============================] - 96s 2s/step - loss: 6.4528e-04\n",
      "Epoch 215/250\n",
      "52/52 [==============================] - 68s 1s/step - loss: 6.2508e-04\n",
      "Epoch 216/250\n",
      "52/52 [==============================] - 69s 1s/step - loss: 6.0614e-04\n",
      "Epoch 217/250\n",
      "52/52 [==============================] - 132s 3s/step - loss: 5.8723e-04\n",
      "Epoch 218/250\n",
      "52/52 [==============================] - 149s 3s/step - loss: 5.6983e-04\n",
      "Epoch 219/250\n",
      "52/52 [==============================] - 99s 2s/step - loss: 5.5228e-04\n",
      "Epoch 220/250\n",
      "52/52 [==============================] - 67s 1s/step - loss: 5.3637e-04\n",
      "Epoch 221/250\n",
      "52/52 [==============================] - 68s 1s/step - loss: 5.1957e-04\n",
      "Epoch 222/250\n",
      "52/52 [==============================] - 68s 1s/step - loss: 5.1580e-04\n",
      "Epoch 223/250\n",
      "52/52 [==============================] - 68s 1s/step - loss: 5.0387e-04\n",
      "Epoch 224/250\n",
      "52/52 [==============================] - 68s 1s/step - loss: 6.9124e-04\n",
      "Epoch 225/250\n",
      "52/52 [==============================] - 68s 1s/step - loss: 0.0010\n",
      "Epoch 226/250\n",
      "52/52 [==============================] - 68s 1s/step - loss: 0.0018\n",
      "Epoch 227/250\n",
      "52/52 [==============================] - 68s 1s/step - loss: 0.0063\n",
      "Epoch 228/250\n",
      "52/52 [==============================] - 68s 1s/step - loss: 0.5825\n",
      "Epoch 229/250\n",
      "52/52 [==============================] - 68s 1s/step - loss: 1.1282\n",
      "Epoch 230/250\n",
      "52/52 [==============================] - 68s 1s/step - loss: 0.6578\n",
      "Epoch 231/250\n",
      "52/52 [==============================] - 68s 1s/step - loss: 0.3673\n",
      "Epoch 232/250\n",
      "52/52 [==============================] - 68s 1s/step - loss: 0.2142\n",
      "Epoch 233/250\n",
      "52/52 [==============================] - 68s 1s/step - loss: 0.1322\n",
      "Epoch 234/250\n",
      "52/52 [==============================] - 68s 1s/step - loss: 0.0877\n",
      "Epoch 235/250\n",
      "52/52 [==============================] - 68s 1s/step - loss: 0.0619\n",
      "Epoch 236/250\n",
      "52/52 [==============================] - 68s 1s/step - loss: 0.0441\n",
      "Epoch 237/250\n",
      "52/52 [==============================] - 68s 1s/step - loss: 0.0315\n",
      "Epoch 238/250\n",
      "52/52 [==============================] - 68s 1s/step - loss: 0.0226\n",
      "Epoch 239/250\n",
      "52/52 [==============================] - 68s 1s/step - loss: 0.0165\n",
      "Epoch 240/250\n",
      "52/52 [==============================] - 68s 1s/step - loss: 0.0115\n",
      "Epoch 241/250\n",
      "52/52 [==============================] - 68s 1s/step - loss: 0.0083\n",
      "Epoch 242/250\n",
      "52/52 [==============================] - 68s 1s/step - loss: 0.0067\n",
      "Epoch 243/250\n",
      "52/52 [==============================] - 68s 1s/step - loss: 0.0058\n",
      "Epoch 244/250\n",
      "52/52 [==============================] - 69s 1s/step - loss: 0.0052\n",
      "Epoch 245/250\n",
      "52/52 [==============================] - 68s 1s/step - loss: 0.0047\n",
      "Epoch 246/250\n",
      "52/52 [==============================] - 68s 1s/step - loss: 0.0044\n",
      "Epoch 247/250\n",
      "52/52 [==============================] - 68s 1s/step - loss: 0.0040\n",
      "Epoch 248/250\n",
      "52/52 [==============================] - 77s 1s/step - loss: 0.0038\n",
      "Epoch 249/250\n",
      "52/52 [==============================] - 82s 2s/step - loss: 0.0036\n",
      "Epoch 250/250\n",
      "52/52 [==============================] - 86s 2s/step - loss: 0.0033\n"
     ]
    }
   ],
   "source": [
    "# آموزش مدل\n",
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits = True)\n",
    "\n",
    "model.compile(optimizer = 'adam', loss = loss)\n",
    "checkpoint_dir = './Model'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"epoch_{epoch}\")\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath = checkpoint_prefix,\n",
    "    save_weights_only = True)\n",
    "\n",
    "history = model.fit(\n",
    "    tsDataSet.batch(\n",
    "        BatchSize,\n",
    "        drop_remainder=True),\n",
    "    epochs = 250,\n",
    "    callbacks = [checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# تولید شعر با استفاده از مدل آموزش داده شده\n",
    "tf.train.latest_checkpoint(checkpoint_dir)\n",
    "model = build_model(EmbeddingDimension, RnningUnits, batch_size = 1)\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\n': 0, ' ': 1, 'ا': 2, 'ب': 3, 'ت': 4, 'ث': 5, 'ج': 6, 'ح': 7, 'خ': 8, 'د': 9, 'ذ': 10, 'ر': 11, 'ز': 12, 'س': 13, 'ش': 14, 'ص': 15, 'ض': 16, 'ط': 17, 'ظ': 18, 'ع': 19, 'غ': 20, 'ف': 21, 'ق': 22, 'ل': 23, 'م': 24, 'ن': 25, 'ه': 26, 'و': 27, 'ٖ': 28, 'پ': 29, 'چ': 30, 'ژ': 31, 'ک': 32, 'گ': 33, 'ی': 34}\n"
     ]
    }
   ],
   "source": [
    "print(Vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n' ' ' 'ا' 'ب' 'ت' 'ث' 'ج' 'ح' 'خ' 'د' 'ذ' 'ر' 'ز' 'س' 'ش' 'ص' 'ض' 'ط'\n",
      " 'ظ' 'ع' 'غ' 'ف' 'ق' 'ل' 'م' 'ن' 'ه' 'و' 'ٖ' 'پ' 'چ' 'ژ' 'ک' 'گ' 'ی']\n"
     ]
    }
   ],
   "source": [
    "print(CharacterMap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "شمست\n",
      "\n",
      "امروز چنان خوان این جا\n",
      "\n",
      "چنان کن که ز بد نالاست برگهست روید\n",
      "من از این پاکتر از می جز ای شه عیسی و من\n",
      "کردی در مرده ان زکار کی گفتم که چه میلوایهم ان چنان قضر\n",
      "به هر دم بدیده استاد دشمن روان شود ز من مهمان شده تا بردی ای دم ضعا را\n",
      "\n",
      "درکش قات اندر و او در فدا چرا\n",
      "\n",
      "چون بدید ان شاه عقل و العش و شور\n",
      "که شد\n"
     ]
    }
   ],
   "source": [
    "Poem = generate_text(model, seed=u\"شمس\")\n",
    "print(Poem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "تبریزی به دست\n",
      "\n",
      "جان میجویدم که مکان کیست\n",
      "\n",
      "جهان بگشا طره و جانش که پی است سر اه کاسه را هست این جا\n",
      "\n",
      "در ان جا بنگر که بود شمع تو شود\n",
      "زان شاه که نیازم نه چون اندر دل ما بستان\n",
      "که صد هزار منزل با قرار ماست\n",
      "\n",
      "خود پیلوی یار کردی ای جو\n",
      "ان مه سرایی تو که فروسیدم خوری\n",
      "چون روی اه تو را این جهان بیستون را\n",
      "\n",
      "نمود ان تا ی\n"
     ]
    }
   ],
   "source": [
    "Poem = generate_text(model, seed=u\"تبریزی\")\n",
    "print(Poem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "سلطان ما\n",
      "\n",
      "در دو جهان لطیف و خوش همچو امیر ما کجا\n",
      "ابروی او گره نشد گر چه که بیدار کند\n",
      "بر خم در هستی در سرسرار ما\n",
      "یک خوش امد و ن غین بترش ز ناعم گفتهای باران در طرب به جز این افراق نیست\n",
      "\n",
      "در مذهر عنابه شوی کم عشق\n",
      "کر من روان شدی دست در شکار نیست\n",
      "\n",
      "گر بحر میپزید و دل بده مینشین منتظانست\n",
      "\n",
      "ان در این عربده بمن هس\n"
     ]
    }
   ],
   "source": [
    "Poem = generate_text(model, seed=u\"سلطان\")\n",
    "print(Poem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "معشوق مطرب مسکین که از تو چو گاه نیست\n",
      "\n",
      "هر کس که دیده باده بلا و بر او براید خدا بود ای دخت طلب پاک\n",
      "که بر از نفکی باشد که به زان برون\n",
      "زخم این چه کارتست سایش غما\n",
      "\n",
      "ای مست رقصان و میمان گشت ز اسمان ما\n",
      "\n",
      "تا بدانی خار امد گر چه برجای چرا\n",
      "\n",
      "ای برند چه بود هر روز چون گل\n",
      "تا چه باشد اقبال بر عاشقان خم شده\n",
      "است\n",
      "که مرا\n"
     ]
    }
   ],
   "source": [
    "Poem = generate_text(model, seed=u\"معشوق\")\n",
    "print(Poem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "خاموش باش مر مهار ما\n",
      "\n",
      "چون بر سر کوی یار کردم به می دست کن\n",
      "با منی همچون من که غیار خوش پنهان چیست\n",
      "\n",
      "شمس تبریزی به خواندند بر رخ اندر دل پرو\n",
      "زان کس که دریا در گوش تو اندر نباشد مرا\n",
      "\n",
      "را چو من\n",
      "گیری به من ندارم چه خوش بود به خدا\n",
      "\n",
      "چو جان زار بلادیده با خدا گوید\n",
      "که جز تو هیچ ندارد هر شب گشته یار بادا\n",
      "\n",
      "به عشق این\n"
     ]
    }
   ],
   "source": [
    "Poem = generate_text(model, seed=u\"خاموش\")\n",
    "print(Poem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "نگاره خود را این نعره در او چون کفرین کیده\n",
      "سیاهیت کاری نگرید این راه بیارگو و برون\n",
      "در پی من هستی ما را مرا یاد ه ای برا\n",
      "\n",
      "ای روز تو به دست برست\n",
      "\n",
      "به گرد سنگ شیران شد سبق از ان دیده بار را\n",
      "\n",
      "گر زان که نگر که نیایم چو کر و سینه برخاست\n",
      "\n",
      "تو شاخی کن همچو بوی ای در و این بیجان\n",
      "اهنی سرد را پیشتر از شکر ما\n",
      "\n",
      "این هم\n"
     ]
    }
   ],
   "source": [
    "Poem = generate_text(model, seed=u\"نگار\")\n",
    "print(Poem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "عاشق ناگهانست\n",
      "\n",
      "زین را که شیرین کند بر تو لاله داران را\n",
      "\n",
      "چنین باشد ان نقص کشیده بدین که را\n",
      "ای عقل باشد حال او خوش لقا\n",
      "\n",
      "کوه طور و دفتم بیاباری را\n",
      "\n",
      "بده بر خمشیره ندارم چه خوش بود به خدا\n",
      "\n",
      "چو شیر پنجه نهد بر شکسته اهوی خویش\n",
      "که ای عزیز شاد جان افرو و سازی شدست\n",
      "\n",
      "تریا ندام عشق شمس دین\n",
      "روز تو را و یفنی هر جا\n",
      "\n",
      "هر\n"
     ]
    }
   ],
   "source": [
    "Poem = generate_text(model, seed=u\"عاشق\")\n",
    "print(Poem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "بهار بیداران ز کجا\n",
      "\n",
      "طفل سلام سلطان و مستعل و تاسما\n",
      "\n",
      "خاموش و تلقینا فی الکاف الصبا\n",
      "\n",
      "لم ترزان ان در سر در و دینار اینست\n",
      "\n",
      "من از میدان شمس دین ان قصه تو بیذاب\n",
      "\n",
      "با ماییم ز تو چون زوسفین مایما\n",
      "\n",
      "چون صباح مرد تهبیر شنی سرنا\n",
      "بی تو در حجات مطلب گردوی ان سینه جان حریفان\n",
      "جان یار که این خانه ت\n",
      "\n",
      "دل ار دل بیشین خواجه \n"
     ]
    }
   ],
   "source": [
    "Poem = generate_text(model, seed=u\"بهار\")\n",
    "print(Poem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Poem = generate_text(model, seed=u\"جان\")\n",
    "print(Poem)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "metadata": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
